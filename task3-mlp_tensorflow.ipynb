{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Spring 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 3: Multilayer Perceptron (MLP)\n",
    "This is the third part of the assignment. You will get to implement MLP using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utils.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3072) (1000, 3072) (10000, 3072) (100, 3072)\n",
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n",
      "Development data shape: (100, 3072)\n",
      "Development data shape (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-100 data.\n",
    "label_map, X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa_w8ZHbtrqY"
   },
   "source": [
    "## Part 1: Tensorflow MLP\n",
    "In this part, you will use tensorflow modules to implement a MLP. We provide a demo of a two-layer net, of which style is referred to https://www.tensorflow.org/guide/keras, and https://www.tensorflow.org/guide/eager. \n",
    "\n",
    "You need to implement a multi-layer with 3 layers in a similar style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Two-layer MLP in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "-0xQOzpdtrqZ",
    "outputId": "212defc2-2cd6-406f-affe-9c6c7367e87f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: valid acc = 0.07800000160932541\n",
      "epoch 2: valid acc = 0.10499999672174454\n",
      "epoch 3: valid acc = 0.09200000017881393\n",
      "epoch 4: valid acc = 0.12800000607967377\n",
      "epoch 5: valid acc = 0.12200000137090683\n",
      "epoch 6: valid acc = 0.14499999582767487\n",
      "epoch 7: valid acc = 0.16699999570846558\n",
      "epoch 8: valid acc = 0.1770000010728836\n",
      "epoch 9: valid acc = 0.17499999701976776\n",
      "epoch 10: valid acc = 0.20100000500679016\n",
      "epoch 11: valid acc = 0.21699999272823334\n",
      "epoch 12: valid acc = 0.22499999403953552\n",
      "test acc = 0.22709999978542328\n"
     ]
    }
   ],
   "source": [
    "## Demo: Two-layer net in tensorflow (eager execution mode)\n",
    "hidden_dim = 300\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "# define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = tf.Variable(1e-2*np.random.rand(3072, hidden_dim).astype('float32'))\n",
    "        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, 20).astype('float32'))\n",
    "        self.b2 = tf.Variable(np.zeros((20,)).astype('float32'))\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n",
    "        out = tf.matmul(h1, self.W2) + self.b2\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,20))\n",
    "    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2)\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n",
    "\n",
    "# calculate gradients for all variables using tf.GradientTape\n",
    "def grad(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2])\n",
    "\n",
    "# calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 49000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 12\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n",
    "        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n",
    "        \n",
    "        grads = grad(model, x_tf, y_tf, reg_tf)\n",
    "        #optimization based on calculated gradients \n",
    "        optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2]))\n",
    "\n",
    "    x_tf = tf.Variable(X_val, dtype = tf.float32)\n",
    "    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    val_acc = accuracy.numpy()\n",
    "    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "\n",
    "x_tf = tf.Variable(X_test, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_test, dtype = tf.uint8)\n",
    "accuracy = eval_acc(model, x_tf, y_tf)\n",
    "test_acc = accuracy.numpy()\n",
    "print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deeper Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmSduBmytrqb"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Create your MLP in tensorflow. Since you are going to create a deeper neural network, it is recommended to use \"list\" to store your network parameters (weights and bias). Consider using a loop to create your MLP network. Hint: Copy the above code and make necessary changes in model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: valid acc = 0.08900000154972076\n",
      "epoch 2: valid acc = 0.08399999886751175\n",
      "epoch 3: valid acc = 0.13500000536441803\n",
      "epoch 4: valid acc = 0.16599999368190765\n",
      "epoch 5: valid acc = 0.18700000643730164\n",
      "epoch 6: valid acc = 0.20600000023841858\n",
      "epoch 7: valid acc = 0.2409999966621399\n",
      "epoch 8: valid acc = 0.2759999930858612\n",
      "epoch 9: valid acc = 0.29899999499320984\n",
      "epoch 10: valid acc = 0.328000009059906\n",
      "epoch 11: valid acc = 0.34299999475479126\n",
      "epoch 12: valid acc = 0.35199999809265137\n",
      "test acc = 0.32420000433921814\n"
     ]
    }
   ],
   "source": [
    "# Create you MLP using TensorFlow functions.\n",
    "hidden_dim = 300\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "# define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = tf.Variable(1e-2*np.random.rand(3072, hidden_dim).astype('float32'))\n",
    "        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        \n",
    "        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, hidden_dim).astype('float32'))\n",
    "        self.b2 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        \n",
    "        self.W3 = tf.Variable(1e-2*np.random.rand(hidden_dim, 20).astype('float32'))\n",
    "        self.b3 = tf.Variable(np.zeros((20,)).astype('float32'))\n",
    "        \n",
    "        self.params = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n",
    "        \n",
    "        h2 = tf.nn.relu(tf.matmul(h1, self.W2) + self.b2)\n",
    "        \n",
    "        out = tf.matmul(h2, self.W3) + self.b3\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function (Note that in eager execution, loss must be in a function)\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= out, labels=tf.one_hot(targets,20))\n",
    "    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2) + tf.nn.l2_loss(model.W3)\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss\n",
    "\n",
    "# calculate gradients for all variables using tf.GradientTape\n",
    "def grad(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3])\n",
    "\n",
    "# calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.cast(tf.argmax(model(inputs),1), tf.uint8))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 49000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 12\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batch):\n",
    "        batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "        x_tf = tf.Variable(batch_xs, dtype = tf.float32)\n",
    "        y_tf = tf.Variable(batch_ys, dtype = tf.uint8)\n",
    "        \n",
    "        grads = grad(model, x_tf, y_tf, reg_tf)\n",
    "        #optimization based on calculated gradients \n",
    "        optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3]))\n",
    "\n",
    "    x_tf = tf.Variable(X_val, dtype = tf.float32)\n",
    "    y_tf = tf.Variable(y_val, dtype = tf.uint8)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    val_acc = accuracy.numpy()\n",
    "    print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "\n",
    "x_tf = tf.Variable(X_test, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_test, dtype = tf.uint8)\n",
    "accuracy = eval_acc(model, x_tf, y_tf)\n",
    "test_acc = accuracy.numpy()\n",
    "print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to TensorFlow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when the network structure becomes larger, it is hard to handle variables from every layer. Here we introduce the `tf.keras` tool to build the network in a much simpler way. You may want to use it in your project.\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: Follow this official example: https://www.tensorflow.org/datasets/keras_example#step_2_create_and_train_the_model to build an MLP and train it. \n",
    "\n",
    "*You should keep the same optimizer (SGD) and loss function (cross entropy) as in the previous task.*\n",
    "\n",
    "**Note:** Since we want to use our own dataset, we will not use the `tfds.load` method to load the data this time. \n",
    "\n",
    "You need to check the usage of [`model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and feed the model with our own data.\n",
    "\n",
    "**Tips:**\n",
    "* Softmax is also registered as a layer operation in tf.keras.\n",
    "* You can use `model.summary()` to visualize the model after you build it.\n",
    "* Use `verbose=2` in `model.fit()` to get similar training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "490/490 - 2s - loss: 10.2558 - sparse_categorical_accuracy: 0.0617 - val_loss: 10.2115 - val_sparse_categorical_accuracy: 0.0710\n",
      "Epoch 2/12\n",
      "490/490 - 2s - loss: 10.1721 - sparse_categorical_accuracy: 0.0750 - val_loss: 10.1331 - val_sparse_categorical_accuracy: 0.0780\n",
      "Epoch 3/12\n",
      "490/490 - 2s - loss: 10.0972 - sparse_categorical_accuracy: 0.0799 - val_loss: 10.0569 - val_sparse_categorical_accuracy: 0.0860\n",
      "Epoch 4/12\n",
      "490/490 - 2s - loss: 10.0233 - sparse_categorical_accuracy: 0.0849 - val_loss: 9.9778 - val_sparse_categorical_accuracy: 0.0960\n",
      "Epoch 5/12\n",
      "490/490 - 2s - loss: 9.9468 - sparse_categorical_accuracy: 0.0926 - val_loss: 9.9078 - val_sparse_categorical_accuracy: 0.0980\n",
      "Epoch 6/12\n",
      "490/490 - 2s - loss: 9.8746 - sparse_categorical_accuracy: 0.0970 - val_loss: 9.8335 - val_sparse_categorical_accuracy: 0.1040\n",
      "Epoch 7/12\n",
      "490/490 - 2s - loss: 9.8041 - sparse_categorical_accuracy: 0.1002 - val_loss: 9.7641 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 8/12\n",
      "490/490 - 2s - loss: 9.7339 - sparse_categorical_accuracy: 0.1039 - val_loss: 9.6940 - val_sparse_categorical_accuracy: 0.1110\n",
      "Epoch 9/12\n",
      "490/490 - 2s - loss: 9.6659 - sparse_categorical_accuracy: 0.1060 - val_loss: 9.6304 - val_sparse_categorical_accuracy: 0.1080\n",
      "Epoch 10/12\n",
      "490/490 - 2s - loss: 9.5984 - sparse_categorical_accuracy: 0.1083 - val_loss: 9.5659 - val_sparse_categorical_accuracy: 0.1060\n",
      "Epoch 11/12\n",
      "490/490 - 2s - loss: 9.5315 - sparse_categorical_accuracy: 0.1105 - val_loss: 9.4935 - val_sparse_categorical_accuracy: 0.1170\n",
      "Epoch 12/12\n",
      "490/490 - 2s - loss: 9.4652 - sparse_categorical_accuracy: 0.1129 - val_loss: 9.4318 - val_sparse_categorical_accuracy: 0.1110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb797402a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# TODO: build the model with tf.keras.models.Sequential\n",
    "\n",
    "model3 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256,activation='relu',kernel_regularizer = tf.keras.regularizers.l2()),\n",
    "  tf.keras.layers.Dense(256,activation='relu',kernel_regularizer = tf.keras.regularizers.l2()),\n",
    "  tf.keras.layers.Dense(20, activation='softmax')\n",
    "])\n",
    "\n",
    "# END of your code\n",
    "#############################################################\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# TODO: compile the model, set optimizer and loss\n",
    "model3.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=5e-4),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "# END of your code\n",
    "#############################################################\n",
    "\n",
    "#############################################################\n",
    "# TODO: train the model with our own dataset\n",
    "\n",
    "model3.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=12,\n",
    "    verbose=2,\n",
    "    batch_size = 100,\n",
    "    validation_data=(X_val, y_val))\n",
    "# END of your code\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: t-SNE (optional, bonus +5 points)\n",
    "\n",
    "t-SNE is is a machine learning algorithm for nonlinear dimensionality reduction developed by Geoffrey Hinton and Laurens van der Maaten. It is also a good way of visualizing high-dimensional data in 2D. We show its application for CIFAR100. Later it will be re-used in a CNN network. Experimenting with t-SNE can be fun. One thing to try is to visualize the output of each layer of MLP to observe the differences.\n",
    "\n",
    "<p style=\"line-height: 1.2;\">[1] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of Machine Learning Research 9.Nov (2008): 2579-2605.</p>\n",
    "<p style=\"line-height: 1.2;\">[2] Adaptive learning rate scheme by Jacobs https://www.willamette.edu/~gorr/classes/cs449/Momentum/deltabardelta.html</p>\n",
    "<p style=\"line-height: 1.2;\">[3] http://cs.stanford.edu/people/karpathy/cnnembed/</p>\n",
    "<p style=\"line-height: 1.2;\">[4] How to Use t-SNE Effectively, with examples.\n",
    " https://distill.pub/2016/misread-tsne</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test\n",
    "from utils.classifiers.mlp import MLP\n",
    "from utils.features.tsne import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-100 data.\n",
    "label_map, X_train, y_train, X_test, y_test = load_data()\n",
    "X_train = X_train.reshape([50000,3,32,32]).transpose((0,2,3,1))\n",
    "X_test = X_test.reshape([10000,3,32,32]).transpose((0,2,3,1))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "# Test data: 10000 samples from original test set: 1~10000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_dev = X_dev.astype(np.float32) - mean_image.astype(np.float32)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development labels shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tSNE of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_select = np.random.choice(10000, 500, replace=False)\n",
    "X = X_test[random_select,:,:,0].reshape(500,1024).astype('float')/255.0\n",
    "tic = time.time()\n",
    "Y = tsne(X, low_dim=2, perplexity=30.0)\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize tSNE of original data\n",
    "labels = y_test[random_select]\n",
    "colors = np.random.rand(20,3)\n",
    "color_labels = [colors[int(i)] for i in labels.tolist()]\n",
    "plt.scatter(Y[:,0], Y[:,1], 20, color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tSNE of data after two hidden layers\n",
    "\n",
    "Do visualization of the tSNE of data after going through MLP. In the visualization result, you should find that in comparison with the tSNE of original data where all data points mess up with each other, tSNE of data after two-layer networks would be shown as multiple clusters in a 2D panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "model = MLP(input_dim=3072, hidden_dims=[100], num_classes=20, reg=0.1, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 200\n",
    "lr = 1e-3\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Visualize data that is passed through MLP model defined above using tSNE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run tSNE\n",
    "X = X_test[random_select]\n",
    "tic = time.time()\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: Pass data through affine and dense layers (model.layers) and then \n",
    "# apply softmax to obtain output of the MLP model.\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tSNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "# Hint: See tSNE visualization of original data\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Try tuning the parameters of tSNE, do visualization of the new tSNE of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the parameter, show the results.\n",
    "# run tSNE\n",
    "X = X_test[random_select]\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "print(\"it takes {} seconds\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tSNE 2D representation of data after two hidden layers\n",
    "#############################################################################\n",
    "#                          START OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
