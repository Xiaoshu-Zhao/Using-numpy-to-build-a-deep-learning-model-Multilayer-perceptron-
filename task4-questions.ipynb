{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Spring 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: In softmax classifier, we can think cross entropy is calculating the distance between the distribution created by softmax classifier and real distribution, which is the label y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: The difference between multi-class and binary logistic regression is that the class number is different. In binary logistic, there is just two class so its y label just has 0 and 1. However, multi-class regression has more than two class, and it will show the probability for each class at end.\n",
    "   \n",
    "   We can combine multiple binary logistic regression classifiers to be a multi-class logistic regression classifier. Firstly, we use a binary classifier to choose which \"big binary class\" dose input data belong to, then we use another binary classifier of this \"big class\" to choose which \"second binary class\" dose it belong to. We keep doing it until we find a real class for input data.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "Sometimes we would like to use Leaky ReLU instead of ReLU, why?\n",
    "\n",
    "   Your answer: when we are doing backward, we did take gradient. When it is smaller than one, it may be closed to zero after many times of taking gradient. However, ReLU did not have this problem, it is a combination of two linear function and it will be very stable. \n",
    "   \n",
    "   ReLU still has a problem that it may turn all the value that smaller than 0 to 0. It may \"kill\" some neurons. However, Leaky ReLU can solve this problem. The output can be a very small number but bigger than 0 when input x is smaller than 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "We have used **batch_size** in our training function. Why don't we simply feed all of the data to the network at once, but feed the data in batches? Please raise some reasons.\n",
    "\n",
    "   Your answer: 1. Memory limit, when input is very large. The running time will be very long.\n",
    "   2. It can better deal with non-convex loss functions \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: when i run my model without any adjustment, my result is 0.28. then I used \"Hyperparameter tuning\". I turn down the batch size and change epoch to 13(after some try), then my acc became 0.32. Then I adjust the reg and change the learning decay to 0.98. Then my acc become 0.34.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[fill in here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
